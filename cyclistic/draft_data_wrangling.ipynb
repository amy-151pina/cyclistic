{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5337aa33",
   "metadata": {},
   "source": [
    "# 2 - PREPARE\n",
    "\n",
    "In this section I will import the monthly datasets and concatenate them to create a single DataFrame called 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "810518e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e732e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing 1 csv at a time\n",
    "desktop_path1 = \"/Users/amylee/Desktop/raw_data/202401-divvy-tripdata.csv\"\n",
    "df1 = pd.read_csv(desktop_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42ceaa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1D650626C8C899A</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-12 15:30:27</td>\n",
       "      <td>2024-01-12 15:37:59</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.903267</td>\n",
       "      <td>-87.634737</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EECD38BDB25BFCB0</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-08 15:45:46</td>\n",
       "      <td>2024-01-08 15:52:59</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.902937</td>\n",
       "      <td>-87.634440</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F4A9CE78061F17F7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-27 12:27:19</td>\n",
       "      <td>2024-01-27 12:35:19</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.902951</td>\n",
       "      <td>-87.634470</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0A0D9E15EE50B171</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-29 16:26:17</td>\n",
       "      <td>2024-01-29 16:56:06</td>\n",
       "      <td>Wells St &amp; Randolph St</td>\n",
       "      <td>TA1305000030</td>\n",
       "      <td>Larrabee St &amp; Webster Ave</td>\n",
       "      <td>13193</td>\n",
       "      <td>41.884295</td>\n",
       "      <td>-87.633963</td>\n",
       "      <td>41.921822</td>\n",
       "      <td>-87.644140</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33FFC9805E3EFF9A</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-31 05:43:23</td>\n",
       "      <td>2024-01-31 06:09:35</td>\n",
       "      <td>Lincoln Ave &amp; Waveland Ave</td>\n",
       "      <td>13253</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.948797</td>\n",
       "      <td>-87.675278</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           started_at             ended_at  \\\n",
       "0  C1D650626C8C899A  electric_bike  2024-01-12 15:30:27  2024-01-12 15:37:59   \n",
       "1  EECD38BDB25BFCB0  electric_bike  2024-01-08 15:45:46  2024-01-08 15:52:59   \n",
       "2  F4A9CE78061F17F7  electric_bike  2024-01-27 12:27:19  2024-01-27 12:35:19   \n",
       "3  0A0D9E15EE50B171   classic_bike  2024-01-29 16:26:17  2024-01-29 16:56:06   \n",
       "4  33FFC9805E3EFF9A   classic_bike  2024-01-31 05:43:23  2024-01-31 06:09:35   \n",
       "\n",
       "           start_station_name start_station_id           end_station_name  \\\n",
       "0           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "1           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "2           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "3      Wells St & Randolph St     TA1305000030  Larrabee St & Webster Ave   \n",
       "4  Lincoln Ave & Waveland Ave            13253   Kingsbury St & Kinzie St   \n",
       "\n",
       "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
       "0   KA1503000043  41.903267 -87.634737  41.889177 -87.638506        member  \n",
       "1   KA1503000043  41.902937 -87.634440  41.889177 -87.638506        member  \n",
       "2   KA1503000043  41.902951 -87.634470  41.889177 -87.638506        member  \n",
       "3          13193  41.884295 -87.633963  41.921822 -87.644140        member  \n",
       "4   KA1503000043  41.948797 -87.675278  41.889177 -87.638506        member  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show first 5 rows\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc15a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path to your data directory\n",
    "base_path = \"/Users/amylee/Desktop/raw_data/\" \n",
    "\n",
    "# Create an empty dictionary to store the DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate through months 1 to 11\n",
    "for month in range(1, 12):\n",
    "    # Construct the filename for each month\n",
    "    filename = f\"2024{month:02d}-divvy-tripdata.csv\"  # Pad month with leading zero (e.g., '01' for Jan)\n",
    "    filepath = os.path.join(base_path, filename)\n",
    "\n",
    "    # Read csv file into a dataframe\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        dataframes[f\"df{month}\"] = df  # Store the DataFrame in the dictionary\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc52f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type           started_at             ended_at  \\\n",
      "0  C1D650626C8C899A  electric_bike  2024-01-12 15:30:27  2024-01-12 15:37:59   \n",
      "1  EECD38BDB25BFCB0  electric_bike  2024-01-08 15:45:46  2024-01-08 15:52:59   \n",
      "2  F4A9CE78061F17F7  electric_bike  2024-01-27 12:27:19  2024-01-27 12:35:19   \n",
      "3  0A0D9E15EE50B171   classic_bike  2024-01-29 16:26:17  2024-01-29 16:56:06   \n",
      "4  33FFC9805E3EFF9A   classic_bike  2024-01-31 05:43:23  2024-01-31 06:09:35   \n",
      "\n",
      "           start_station_name start_station_id           end_station_name  \\\n",
      "0           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
      "1           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
      "2           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
      "3      Wells St & Randolph St     TA1305000030  Larrabee St & Webster Ave   \n",
      "4  Lincoln Ave & Waveland Ave            13253   Kingsbury St & Kinzie St   \n",
      "\n",
      "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
      "0   KA1503000043  41.903267 -87.634737  41.889177 -87.638506        member  \n",
      "1   KA1503000043  41.902937 -87.634440  41.889177 -87.638506        member  \n",
      "2   KA1503000043  41.902951 -87.634470  41.889177 -87.638506        member  \n",
      "3          13193  41.884295 -87.633963  41.921822 -87.644140        member  \n",
      "4   KA1503000043  41.948797 -87.675278  41.889177 -87.638506        member  \n"
     ]
    }
   ],
   "source": [
    "# Now you have DataFrames named df1, df2, ..., df11 in the 'dataframes' dictionary\n",
    "print(dataframes[\"df1\"].head())  # Example: Access and display the first DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f3bc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the DataFrames from the dictionary\n",
    "df1 = dataframes['df1']\n",
    "df2 = dataframes['df2']\n",
    "df3 = dataframes['df3']\n",
    "df4 = dataframes['df4']\n",
    "df5 = dataframes['df5']\n",
    "df6 = dataframes['df6']\n",
    "df7 = dataframes['df7']\n",
    "df8 = dataframes['df8']\n",
    "df9 = dataframes['df9']\n",
    "df10 = dataframes['df10']\n",
    "df11 = dataframes['df11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67ea6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all dataframes\n",
    "dataframes_list = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67be1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dataframes_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fa3cfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1D650626C8C899A</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-12 15:30:27</td>\n",
       "      <td>2024-01-12 15:37:59</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.903267</td>\n",
       "      <td>-87.634737</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EECD38BDB25BFCB0</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-08 15:45:46</td>\n",
       "      <td>2024-01-08 15:52:59</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.902937</td>\n",
       "      <td>-87.634440</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F4A9CE78061F17F7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-27 12:27:19</td>\n",
       "      <td>2024-01-27 12:35:19</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.902951</td>\n",
       "      <td>-87.634470</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0A0D9E15EE50B171</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-29 16:26:17</td>\n",
       "      <td>2024-01-29 16:56:06</td>\n",
       "      <td>Wells St &amp; Randolph St</td>\n",
       "      <td>TA1305000030</td>\n",
       "      <td>Larrabee St &amp; Webster Ave</td>\n",
       "      <td>13193</td>\n",
       "      <td>41.884295</td>\n",
       "      <td>-87.633963</td>\n",
       "      <td>41.921822</td>\n",
       "      <td>-87.644140</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33FFC9805E3EFF9A</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-31 05:43:23</td>\n",
       "      <td>2024-01-31 06:09:35</td>\n",
       "      <td>Lincoln Ave &amp; Waveland Ave</td>\n",
       "      <td>13253</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.948797</td>\n",
       "      <td>-87.675278</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           started_at             ended_at  \\\n",
       "0  C1D650626C8C899A  electric_bike  2024-01-12 15:30:27  2024-01-12 15:37:59   \n",
       "1  EECD38BDB25BFCB0  electric_bike  2024-01-08 15:45:46  2024-01-08 15:52:59   \n",
       "2  F4A9CE78061F17F7  electric_bike  2024-01-27 12:27:19  2024-01-27 12:35:19   \n",
       "3  0A0D9E15EE50B171   classic_bike  2024-01-29 16:26:17  2024-01-29 16:56:06   \n",
       "4  33FFC9805E3EFF9A   classic_bike  2024-01-31 05:43:23  2024-01-31 06:09:35   \n",
       "\n",
       "           start_station_name start_station_id           end_station_name  \\\n",
       "0           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "1           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "2           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "3      Wells St & Randolph St     TA1305000030  Larrabee St & Webster Ave   \n",
       "4  Lincoln Ave & Waveland Ave            13253   Kingsbury St & Kinzie St   \n",
       "\n",
       "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
       "0   KA1503000043  41.903267 -87.634737  41.889177 -87.638506        member  \n",
       "1   KA1503000043  41.902937 -87.634440  41.889177 -87.638506        member  \n",
       "2   KA1503000043  41.902951 -87.634470  41.889177 -87.638506        member  \n",
       "3          13193  41.884295 -87.633963  41.921822 -87.644140        member  \n",
       "4   KA1503000043  41.948797 -87.675278  41.889177 -87.638506        member  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the first few rows of the combined DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21cfccda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5682196 entries, 0 to 5682195\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 563.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#high level view of dataframe info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b9c74",
   "metadata": {},
   "source": [
    "### PREPARE SUMMARY\n",
    "\n",
    "In this section I uploaded the monthly csv data to my working directory. I read the csv files in as individual dataframes then combined them into 1 combined dataframe called 'df'.\n",
    "\n",
    "The dataframe has 12 columns with 5,682,196 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c61bd3",
   "metadata": {},
   "source": [
    "# PROCESS\n",
    "\n",
    "In this section I will complete the following key tasks:\n",
    "     1.      Check the data for errors.\n",
    "     2.      Transform the data so I can work with it effectively.\n",
    "     3.      Document the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2775176",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains when parsing with format \"%Y-%m-%d %H:%M:%S\": \".289\", at position 1694242. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1112\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m         values \u001b[38;5;241m=\u001b[39m convert_listlike(arg\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1113\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:488\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[1;32m    490\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[1;32m    491\u001b[0m     arg,\n\u001b[1;32m    492\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:519\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    509\u001b[0m     arg,\n\u001b[1;32m    510\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001b[0;32mstrptime.pyx:534\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:359\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unconverted data remains when parsing with format \"%Y-%m-%d %H:%M:%S\": \".289\", at position 1694242. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "df['started_at'] = pd.to_datetime(df['started_at'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc6ba016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id                      CDE6023BE6B11D2F\n",
      "rideable_type                   electric_bike\n",
      "started_at            2024-06-11 17:20:06.289\n",
      "ended_at              2024-06-11 17:21:39.464\n",
      "start_station_name                        NaN\n",
      "start_station_id                          NaN\n",
      "end_station_name                          NaN\n",
      "end_station_id                            NaN\n",
      "start_lat                               41.89\n",
      "start_lng                              -87.65\n",
      "end_lat                                 41.89\n",
      "end_lng                                -87.65\n",
      "member_casual                          casual\n",
      "Name: 1694242, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#investigating problematic row - started_at does not conform to datetime format\n",
    "problem_row = df.loc[1694242]\n",
    "print(problem_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4437978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations with fractional seconds in 'started_at' column: 3987954\n"
     ]
    }
   ],
   "source": [
    "#count observations with fractional seconds in 'started_at'\n",
    "count_frxnal_seconds = df['started_at'].str.contains(r'\\.\\d+').sum()\n",
    "print(f\"Number of observations with fractional seconds in 'started_at' column: {count_frxnal_seconds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d99eba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of dataset with fractional seconds: 70.18332348972123\n"
     ]
    }
   ],
   "source": [
    "#ratio of dataset that has fractional seconds\n",
    "ratio_frxnal_seconds = count_frxnal_seconds/(len(df))*100\n",
    "print(f\"Percent of dataset with fractional seconds: {ratio_frxnal_seconds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd06ce6",
   "metadata": {},
   "source": [
    "70% of the dataset has fractional seconds. Therefore, I cannot easily convert 'started_at' and 'ended_at' columns to datetime format. I will \n",
    "add .000 to the end of the 30% of the 'started_at'/'ended_at' that did not have fractional seconds and round. \n",
    "\n",
    "***For the purposes of this analysis this type of precision is not necessary. Removing the fractional seconds then converting to datetime would have sufficed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30dfbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fractional_seconds(df):\n",
    "    \"\"\"\n",
    "    Adds '.000' to timestamps in 'started_at' and 'ended_at' columns that do not alread have fractional seconds.\n",
    "    \n",
    "    Args:\n",
    "        df: The pandas DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        The DataFrame with modified timestamps.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['started_at'] = df['started_at'].apply(lambda x: x+'.000' if not '.' in x else x)\n",
    "    df['ended_at'] = df['ended_at'].apply(lambda x: x+'.000' if not '.' in x else x)\n",
    "    return df\n",
    "\n",
    "#re-establish df\n",
    "df = add_fractional_seconds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08c6d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 'started_at' and 'ended_at' columns to datetime\n",
    "df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "df['ended_at'] = pd.to_datetime(df['ended_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a43d3462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5682196 entries, 0 to 5682195\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   ride_id             object        \n",
      " 1   rideable_type       object        \n",
      " 2   started_at          datetime64[ns]\n",
      " 3   ended_at            datetime64[ns]\n",
      " 4   start_station_name  object        \n",
      " 5   start_station_id    object        \n",
      " 6   end_station_name    object        \n",
      " 7   end_station_id      object        \n",
      " 8   start_lat           float64       \n",
      " 9   start_lng           float64       \n",
      " 10  end_lat             float64       \n",
      " 11  end_lng             float64       \n",
      " 12  member_casual       object        \n",
      "dtypes: datetime64[ns](2), float64(4), object(7)\n",
      "memory usage: 563.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#calculate and create a column called 'ride_length'\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "407f516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Value Summary:\n",
      "                     Null Count  Null Percentage\n",
      "ride_id                      0             0.00\n",
      "rideable_type                0             0.00\n",
      "started_at                   0             0.00\n",
      "ended_at                     0             0.00\n",
      "start_station_name     1044760            18.39\n",
      "start_station_id       1044760            18.39\n",
      "end_station_name       1073877            18.90\n",
      "end_station_id         1073877            18.90\n",
      "start_lat                    0             0.00\n",
      "start_lng                    0             0.00\n",
      "end_lat                   7101             0.12\n",
      "end_lng                   7101             0.12\n",
      "member_casual                0             0.00\n"
     ]
    }
   ],
   "source": [
    "# count number of null values in each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# calculate the % of null values in each column\n",
    "null_percents = (null_counts/len(df)) * 100\n",
    "\n",
    "#create a DataFrame to display results\n",
    "null_summary = pd.DataFrame({'Null Count': null_counts,\n",
    "                            'Null Percentage': null_percents.round(2)})\n",
    "\n",
    "# print results\n",
    "print(\"Null Value Summary:\\n\", null_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aabbeaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id                     0\n",
      "rideable_type               0\n",
      "started_at                  0\n",
      "ended_at                    0\n",
      "start_station_name    1044760\n",
      "start_station_id      1044760\n",
      "end_station_name      1066776\n",
      "end_station_id        1066776\n",
      "start_lat                   0\n",
      "start_lng                   0\n",
      "end_lat                     0\n",
      "end_lng                     0\n",
      "member_casual               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# drop rows containing null values in end_lat and end_lng columns\n",
    "\n",
    "df = df.dropna(subset=['end_lat', 'end_lng'])\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8f161d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 13 entries, ride_id to member_casual\n",
      "Series name: None\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "13 non-null     int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 764.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "null_counts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e862a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_decimal_places(df, col_name):\n",
    "  \"\"\"\n",
    "  Counts the number of observations with each amount of decimal places \n",
    "  in the specified column of a DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df: The pandas DataFrame.\n",
    "    col_name: The name of the column to analyze.\n",
    "\n",
    "  Returns:\n",
    "    A pandas Series containing the counts of observations for each \n",
    "    number of decimal places.\n",
    "  \"\"\"\n",
    "\n",
    "  # Extract the decimal part of the column\n",
    "  decimal_parts = df['start_lat'].astype(str).str.split('.').str[1] \n",
    "\n",
    "  # Handle cases where there are no decimal places\n",
    "  decimal_parts = decimal_parts.fillna('') \n",
    "\n",
    "  # Count the number of decimal places for each observation\n",
    "  decimal_place_counts = decimal_parts.str.len() \n",
    "\n",
    "  # Count the occurrences of each decimal place count\n",
    "  decimal_place_counts = decimal_place_counts.value_counts()\n",
    "\n",
    "  return decimal_place_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c957ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimal Place Counts for 'start_lat':\n",
      " start_lat\n",
      "6     2126737\n",
      "2      936765\n",
      "9      741556\n",
      "14     540994\n",
      "11     289371\n",
      "5      257011\n",
      "15     163179\n",
      "8      132154\n",
      "10     122541\n",
      "7      122479\n",
      "1      107999\n",
      "13      91300\n",
      "4       42485\n",
      "12        364\n",
      "3         160\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Decimal Place Counts for 'end_lat':\n",
      " end_lat\n",
      "6     2635945\n",
      "2      956626\n",
      "14     572520\n",
      "11     360895\n",
      "5      332355\n",
      "15     163199\n",
      "10     152161\n",
      "13     110333\n",
      "1      110150\n",
      "7      102100\n",
      "8       72748\n",
      "4       52918\n",
      "9       52396\n",
      "12        496\n",
      "3         253\n",
      "Name: count, dtype: int64\n",
      "Decimal Place Counts for 'start_lng':\n",
      " start_lng\n",
      "6     2007678\n",
      "2      998002\n",
      "9      725207\n",
      "14     724408\n",
      "10     423322\n",
      "5      323268\n",
      "8      161715\n",
      "7      108763\n",
      "13      74719\n",
      "4       69639\n",
      "1       46760\n",
      "3       11614\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Decimal Place Counts for 'end_lng':\n",
      " end_lng\n",
      "6     2492149\n",
      "2     1018481\n",
      "14     753292\n",
      "10     523836\n",
      "5      411040\n",
      "8      107357\n",
      "13      93262\n",
      "4       90419\n",
      "7       83584\n",
      "1       48295\n",
      "9       39627\n",
      "3       13753\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_decimal_places(df, col_name):\n",
    "  \"\"\"\n",
    "  Counts the number of observations with each amount of decimal places \n",
    "  in the specified column of a DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df: The pandas DataFrame.\n",
    "    col_name: The name of the column to analyze.\n",
    "\n",
    "  Returns:\n",
    "    A pandas Series containing the counts of observations for each \n",
    "    number of decimal places.\n",
    "  \"\"\"\n",
    "\n",
    "  # Extract the decimal part of the column\n",
    "  decimal_parts = df[col_name].astype(str).str.split('.').str[1] \n",
    "\n",
    "  # Handle cases where there are no decimal places\n",
    "  decimal_parts = decimal_parts.fillna('') \n",
    "\n",
    "  # Count the number of decimal places for each observation\n",
    "  decimal_place_counts = decimal_parts.str.len() \n",
    "\n",
    "  # Count the occurrences of each decimal place count\n",
    "  decimal_place_counts = decimal_place_counts.value_counts()\n",
    "\n",
    "  return decimal_place_counts\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "start_lat_decimal_counts = count_decimal_places(df, 'start_lat')\n",
    "end_lat_decimal_counts = count_decimal_places(df, 'end_lat')\n",
    "\n",
    "start_lng_decimal_counts = count_decimal_places(df, 'start_lng')\n",
    "end_lng_decimal_counts = count_decimal_places(df, 'end_lng')\n",
    "\n",
    "\n",
    "print(\"Decimal Place Counts for 'start_lat':\\n\", start_lat_decimal_counts)\n",
    "print(\"\\nDecimal Place Counts for 'end_lat':\\n\", end_lat_decimal_counts)\n",
    "\n",
    "print(\"Decimal Place Counts for 'start_lng':\\n\", start_lng_decimal_counts)\n",
    "print(\"\\nDecimal Place Counts for 'end_lng':\\n\", end_lng_decimal_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c68b9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decided that for this analysis lat/lng data is sufficient, station name/id is not needed. Therefore I will drop 4 colummns: start_station_name, start_station_id, end_station_name, end_station_id\n",
    "\n",
    "df = df.drop(columns=['start_station_name', 'start_station_id', 'end_station_name', 'end_station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4179b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id          0\n",
      "rideable_type    0\n",
      "started_at       0\n",
      "ended_at         0\n",
      "start_lat        0\n",
      "start_lng        0\n",
      "end_lat          0\n",
      "end_lng          0\n",
      "member_casual    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51546068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "\n",
    "#find duplicate rows based on all columns\n",
    "duplicate_rows = df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42299ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "% of dataset that are duplicates: 0.0\n"
     ]
    }
   ],
   "source": [
    "# count the number of duplicate rows\n",
    "num_duplicates = len(duplicate_rows)\n",
    "\n",
    "# calculate percentage duplicates make up of dataset \n",
    "percent_duplicates = num_duplicates / len(df) * 100\n",
    "\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "print(f\"% of dataset that are duplicates: {percent_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b43d0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously had double imported august data. This was part of trouble shooting. Fixed the original import, this section no longer relevant.\n",
    "# duplicate_rows.groupby(['ride_id'])\n",
    "# print(duplicate_rows.groupby(['ride_id']).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6eadd66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed\n",
    "# check for duplicates\n",
    "\n",
    "# find duplicate rows based on ride_id column\n",
    "# duplicate_ride_ids = df['ride_id'].duplicated(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fa48b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed \n",
    "# filter the DataFrame to include only rows with duplicate 'ride_id'\n",
    "# rows_with_duplicate_ride_ids = df[duplicate_ride_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6317cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed \n",
    "# sort the duplicate rows by 'ride_id'\n",
    "# rows_with_duplicate_ride_ids = rows_with_duplicate_ride_ids.sort_values(by='ride_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d429e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the sorted duplicate rows\n",
    "# print(rows_with_duplicate_ride_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0299ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_duplicate_ride_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8560d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_duplicate_ride_ids.groupby(['ride_id'])\n",
    "# print(rows_with_duplicate_ride_ids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fad4fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_duplicate_ride_ids.groupby(['ride_id'])\n",
    "# print(rows_with_duplicate_ride_ids.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe2a7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_duplicate = df[df['ride_id'] == '00001004F8DF01C8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37068ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the selected rows\n",
    "# print(example_duplicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8515b9",
   "metadata": {},
   "source": [
    "All duplicates seem to be in the month of August. Checking imported dfs to make sure August was not somehow uploaded twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "665d469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed \n",
    "# df8.info()\n",
    "# df8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3adad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed \n",
    "# print(len(df8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91a185",
   "metadata": {},
   "source": [
    "columns 10 (end_lat) and 11 (end_lng) have 754612 non-null values --> the same as number of duplicates listed. Must be a code error.\n",
    "\n",
    "Error identified in cell 20: original import brought in august twice and no september. Fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a92693fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type          started_at            ended_at  \\\n",
      "0  C1D650626C8C899A  electric_bike 2024-01-12 15:30:27 2024-01-12 15:37:59   \n",
      "1  EECD38BDB25BFCB0  electric_bike 2024-01-08 15:45:46 2024-01-08 15:52:59   \n",
      "2  F4A9CE78061F17F7  electric_bike 2024-01-27 12:27:19 2024-01-27 12:35:19   \n",
      "3  0A0D9E15EE50B171   classic_bike 2024-01-29 16:26:17 2024-01-29 16:56:06   \n",
      "4  33FFC9805E3EFF9A   classic_bike 2024-01-31 05:43:23 2024-01-31 06:09:35   \n",
      "\n",
      "   start_lat  start_lng    end_lat    end_lng member_casual     ride_length  \n",
      "0  41.903267 -87.634737  41.889177 -87.638506        member 0 days 00:07:32  \n",
      "1  41.902937 -87.634440  41.889177 -87.638506        member 0 days 00:07:13  \n",
      "2  41.902951 -87.634470  41.889177 -87.638506        member 0 days 00:08:00  \n",
      "3  41.884295 -87.633963  41.921822 -87.644140        member 0 days 00:29:49  \n",
      "4  41.948797 -87.675278  41.889177 -87.638506        member 0 days 00:26:12  \n"
     ]
    }
   ],
   "source": [
    "# calculate ride length and add new column to dataframe\n",
    "\n",
    "df['ride_length'] = (df['ended_at'] - df['started_at'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a26080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>ride_length</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1D650626C8C899A</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-12 15:30:27</td>\n",
       "      <td>2024-01-12 15:37:59</td>\n",
       "      <td>41.903267</td>\n",
       "      <td>-87.634737</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "      <td>0 days 00:07:32</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EECD38BDB25BFCB0</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-08 15:45:46</td>\n",
       "      <td>2024-01-08 15:52:59</td>\n",
       "      <td>41.902937</td>\n",
       "      <td>-87.634440</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "      <td>0 days 00:07:13</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F4A9CE78061F17F7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-27 12:27:19</td>\n",
       "      <td>2024-01-27 12:35:19</td>\n",
       "      <td>41.902951</td>\n",
       "      <td>-87.634470</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "      <td>0 days 00:08:00</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0A0D9E15EE50B171</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-29 16:26:17</td>\n",
       "      <td>2024-01-29 16:56:06</td>\n",
       "      <td>41.884295</td>\n",
       "      <td>-87.633963</td>\n",
       "      <td>41.921822</td>\n",
       "      <td>-87.644140</td>\n",
       "      <td>member</td>\n",
       "      <td>0 days 00:29:49</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33FFC9805E3EFF9A</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-31 05:43:23</td>\n",
       "      <td>2024-01-31 06:09:35</td>\n",
       "      <td>41.948797</td>\n",
       "      <td>-87.675278</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "      <td>0 days 00:26:12</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type          started_at            ended_at  \\\n",
       "0  C1D650626C8C899A  electric_bike 2024-01-12 15:30:27 2024-01-12 15:37:59   \n",
       "1  EECD38BDB25BFCB0  electric_bike 2024-01-08 15:45:46 2024-01-08 15:52:59   \n",
       "2  F4A9CE78061F17F7  electric_bike 2024-01-27 12:27:19 2024-01-27 12:35:19   \n",
       "3  0A0D9E15EE50B171   classic_bike 2024-01-29 16:26:17 2024-01-29 16:56:06   \n",
       "4  33FFC9805E3EFF9A   classic_bike 2024-01-31 05:43:23 2024-01-31 06:09:35   \n",
       "\n",
       "   start_lat  start_lng    end_lat    end_lng member_casual     ride_length  \\\n",
       "0  41.903267 -87.634737  41.889177 -87.638506        member 0 days 00:07:32   \n",
       "1  41.902937 -87.634440  41.889177 -87.638506        member 0 days 00:07:13   \n",
       "2  41.902951 -87.634470  41.889177 -87.638506        member 0 days 00:08:00   \n",
       "3  41.884295 -87.633963  41.921822 -87.644140        member 0 days 00:29:49   \n",
       "4  41.948797 -87.675278  41.889177 -87.638506        member 0 days 00:26:12   \n",
       "\n",
       "  day_of_week  \n",
       "0      Friday  \n",
       "1      Monday  \n",
       "2    Saturday  \n",
       "3      Monday  \n",
       "4   Wednesday  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate day_of_week and add column to dataframe\n",
    "df['day_of_week'] = df['started_at'].dt.day_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ce05a",
   "metadata": {},
   "source": [
    "#### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9967605a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5675095, 11)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73613c23",
   "metadata": {},
   "source": [
    "Save this to your data directory, separately. Uploaded raw data as monthly csvs. Should save derived data in a separate location. This guards against overwriting our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0dcda8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, filename, path):\n",
    "    \"\"\"\n",
    "    Saves a pandas DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        filename: The name of the CSV file.\n",
    "        path: The path to the directory where the file should be saved.\n",
    "    \"\"\"\n",
    "    filepath = f\"{path}/{filename}\" \n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a42955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to a new csv file\n",
    "datapath = '/Users/amylee/google_data_analytics/cyclistic_case_study/cyclistic_data'\n",
    "save_file(df, 'df_cyclistic_clean.csv', datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8cec8615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amylee/google_data_analytics/cyclistic_case_study'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739a027",
   "metadata": {},
   "source": [
    "### PROCESS SUMMARY\n",
    "\n",
    "Data cleaning tasks completed in this section:\n",
    "1. Ensured all columns were in appropriate data format - converted 'started_at' and 'ended_at' to datetime\n",
    "2. Addressed/eliminated null values. \n",
    "a - Dropped rows with null values in 'end_lat' and 'end_lng' columns (~0.01% of dataset).\n",
    "b - Dropped 4 columns: 'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id'. 18% of observations in these columns were missing data. Dataset also contains start/end latitude/longitude so that will be used for geographical analysis. \n",
    "5. Checked for duplicates - none. Identified import error and corrected.\n",
    "\n",
    "I then added columns to calculate ride_length and the day_of_week for each ride."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
