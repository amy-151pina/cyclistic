{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5337aa33",
   "metadata": {},
   "source": [
    "# 2 - PREPARE\n",
    "\n",
    "In this section I will import the monthly datasets and concatenate them to create a single DataFrame called 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "810518e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e732e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing 1 csv at a time\n",
    "desktop_path1 = \"/Users/amylee/Desktop/raw_data/202401-divvy-tripdata.csv\"\n",
    "df1 = pd.read_csv(desktop_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42ceaa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1D650626C8C899A</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-12 15:30:27</td>\n",
       "      <td>2024-01-12 15:37:59</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.903267</td>\n",
       "      <td>-87.634737</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EECD38BDB25BFCB0</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-08 15:45:46</td>\n",
       "      <td>2024-01-08 15:52:59</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.902937</td>\n",
       "      <td>-87.634440</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F4A9CE78061F17F7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-27 12:27:19</td>\n",
       "      <td>2024-01-27 12:35:19</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.902951</td>\n",
       "      <td>-87.634470</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0A0D9E15EE50B171</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-29 16:26:17</td>\n",
       "      <td>2024-01-29 16:56:06</td>\n",
       "      <td>Wells St &amp; Randolph St</td>\n",
       "      <td>TA1305000030</td>\n",
       "      <td>Larrabee St &amp; Webster Ave</td>\n",
       "      <td>13193</td>\n",
       "      <td>41.884295</td>\n",
       "      <td>-87.633963</td>\n",
       "      <td>41.921822</td>\n",
       "      <td>-87.644140</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33FFC9805E3EFF9A</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-31 05:43:23</td>\n",
       "      <td>2024-01-31 06:09:35</td>\n",
       "      <td>Lincoln Ave &amp; Waveland Ave</td>\n",
       "      <td>13253</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.948797</td>\n",
       "      <td>-87.675278</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           started_at             ended_at  \\\n",
       "0  C1D650626C8C899A  electric_bike  2024-01-12 15:30:27  2024-01-12 15:37:59   \n",
       "1  EECD38BDB25BFCB0  electric_bike  2024-01-08 15:45:46  2024-01-08 15:52:59   \n",
       "2  F4A9CE78061F17F7  electric_bike  2024-01-27 12:27:19  2024-01-27 12:35:19   \n",
       "3  0A0D9E15EE50B171   classic_bike  2024-01-29 16:26:17  2024-01-29 16:56:06   \n",
       "4  33FFC9805E3EFF9A   classic_bike  2024-01-31 05:43:23  2024-01-31 06:09:35   \n",
       "\n",
       "           start_station_name start_station_id           end_station_name  \\\n",
       "0           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "1           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "2           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "3      Wells St & Randolph St     TA1305000030  Larrabee St & Webster Ave   \n",
       "4  Lincoln Ave & Waveland Ave            13253   Kingsbury St & Kinzie St   \n",
       "\n",
       "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
       "0   KA1503000043  41.903267 -87.634737  41.889177 -87.638506        member  \n",
       "1   KA1503000043  41.902937 -87.634440  41.889177 -87.638506        member  \n",
       "2   KA1503000043  41.902951 -87.634470  41.889177 -87.638506        member  \n",
       "3          13193  41.884295 -87.633963  41.921822 -87.644140        member  \n",
       "4   KA1503000043  41.948797 -87.675278  41.889177 -87.638506        member  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show first 5 rows\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc15a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path to your data directory\n",
    "base_path = \"/Users/amylee/Desktop/raw_data/\" \n",
    "\n",
    "# Create an empty dictionary to store the DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate through months 1 to 11\n",
    "for month in range(1, 12):\n",
    "    # Construct the filename for each month\n",
    "    filename = f\"2024{month:02d}-divvy-tripdata.csv\"  # Pad month with leading zero (e.g., '01' for Jan)\n",
    "    filepath = os.path.join(base_path, filename)\n",
    "\n",
    "    # Read csv file into a dataframe\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        dataframes[f\"df{month}\"] = df  # Store the DataFrame in the dictionary\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc52f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type           started_at             ended_at  \\\n",
      "0  C1D650626C8C899A  electric_bike  2024-01-12 15:30:27  2024-01-12 15:37:59   \n",
      "1  EECD38BDB25BFCB0  electric_bike  2024-01-08 15:45:46  2024-01-08 15:52:59   \n",
      "2  F4A9CE78061F17F7  electric_bike  2024-01-27 12:27:19  2024-01-27 12:35:19   \n",
      "3  0A0D9E15EE50B171   classic_bike  2024-01-29 16:26:17  2024-01-29 16:56:06   \n",
      "4  33FFC9805E3EFF9A   classic_bike  2024-01-31 05:43:23  2024-01-31 06:09:35   \n",
      "\n",
      "           start_station_name start_station_id           end_station_name  \\\n",
      "0           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
      "1           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
      "2           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
      "3      Wells St & Randolph St     TA1305000030  Larrabee St & Webster Ave   \n",
      "4  Lincoln Ave & Waveland Ave            13253   Kingsbury St & Kinzie St   \n",
      "\n",
      "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
      "0   KA1503000043  41.903267 -87.634737  41.889177 -87.638506        member  \n",
      "1   KA1503000043  41.902937 -87.634440  41.889177 -87.638506        member  \n",
      "2   KA1503000043  41.902951 -87.634470  41.889177 -87.638506        member  \n",
      "3          13193  41.884295 -87.633963  41.921822 -87.644140        member  \n",
      "4   KA1503000043  41.948797 -87.675278  41.889177 -87.638506        member  \n"
     ]
    }
   ],
   "source": [
    "# Now you have DataFrames named df1, df2, ..., df11 in the 'dataframes' dictionary\n",
    "print(dataframes[\"df1\"].head())  # Example: Access and display the first DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f3bc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the DataFrames from the dictionary\n",
    "df1 = dataframes['df1']\n",
    "df2 = dataframes['df2']\n",
    "df3 = dataframes['df3']\n",
    "df4 = dataframes['df4']\n",
    "df5 = dataframes['df5']\n",
    "df6 = dataframes['df6']\n",
    "df7 = dataframes['df7']\n",
    "df8 = dataframes['df8']\n",
    "df9 = dataframes['df9']\n",
    "df10 = dataframes['df10']\n",
    "df11 = dataframes['df11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ea6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all dataframes\n",
    "dataframes_list = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67be1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dataframes_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fa3cfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1D650626C8C899A</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-12 15:30:27</td>\n",
       "      <td>2024-01-12 15:37:59</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.903267</td>\n",
       "      <td>-87.634737</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EECD38BDB25BFCB0</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-08 15:45:46</td>\n",
       "      <td>2024-01-08 15:52:59</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.902937</td>\n",
       "      <td>-87.634440</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F4A9CE78061F17F7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-27 12:27:19</td>\n",
       "      <td>2024-01-27 12:35:19</td>\n",
       "      <td>Wells St &amp; Elm St</td>\n",
       "      <td>KA1504000135</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.902951</td>\n",
       "      <td>-87.634470</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0A0D9E15EE50B171</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-29 16:26:17</td>\n",
       "      <td>2024-01-29 16:56:06</td>\n",
       "      <td>Wells St &amp; Randolph St</td>\n",
       "      <td>TA1305000030</td>\n",
       "      <td>Larrabee St &amp; Webster Ave</td>\n",
       "      <td>13193</td>\n",
       "      <td>41.884295</td>\n",
       "      <td>-87.633963</td>\n",
       "      <td>41.921822</td>\n",
       "      <td>-87.644140</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33FFC9805E3EFF9A</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-01-31 05:43:23</td>\n",
       "      <td>2024-01-31 06:09:35</td>\n",
       "      <td>Lincoln Ave &amp; Waveland Ave</td>\n",
       "      <td>13253</td>\n",
       "      <td>Kingsbury St &amp; Kinzie St</td>\n",
       "      <td>KA1503000043</td>\n",
       "      <td>41.948797</td>\n",
       "      <td>-87.675278</td>\n",
       "      <td>41.889177</td>\n",
       "      <td>-87.638506</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           started_at             ended_at  \\\n",
       "0  C1D650626C8C899A  electric_bike  2024-01-12 15:30:27  2024-01-12 15:37:59   \n",
       "1  EECD38BDB25BFCB0  electric_bike  2024-01-08 15:45:46  2024-01-08 15:52:59   \n",
       "2  F4A9CE78061F17F7  electric_bike  2024-01-27 12:27:19  2024-01-27 12:35:19   \n",
       "3  0A0D9E15EE50B171   classic_bike  2024-01-29 16:26:17  2024-01-29 16:56:06   \n",
       "4  33FFC9805E3EFF9A   classic_bike  2024-01-31 05:43:23  2024-01-31 06:09:35   \n",
       "\n",
       "           start_station_name start_station_id           end_station_name  \\\n",
       "0           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "1           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "2           Wells St & Elm St     KA1504000135   Kingsbury St & Kinzie St   \n",
       "3      Wells St & Randolph St     TA1305000030  Larrabee St & Webster Ave   \n",
       "4  Lincoln Ave & Waveland Ave            13253   Kingsbury St & Kinzie St   \n",
       "\n",
       "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
       "0   KA1503000043  41.903267 -87.634737  41.889177 -87.638506        member  \n",
       "1   KA1503000043  41.902937 -87.634440  41.889177 -87.638506        member  \n",
       "2   KA1503000043  41.902951 -87.634470  41.889177 -87.638506        member  \n",
       "3          13193  41.884295 -87.633963  41.921822 -87.644140        member  \n",
       "4   KA1503000043  41.948797 -87.675278  41.889177 -87.638506        member  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the first few rows of the combined DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21cfccda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5682196 entries, 0 to 5682195\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 563.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#high level view of dataframe info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b9c74",
   "metadata": {},
   "source": [
    "### PREPARE SUMMARY\n",
    "\n",
    "In this section I uploaded the monthly csv data to my working directory. I read the csv files in as individual dataframes then combined them into 1 combined dataframe called 'df'.\n",
    "\n",
    "The dataframe has 12 columns with 5,682,196 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c61bd3",
   "metadata": {},
   "source": [
    "# PROCESS\n",
    "\n",
    "In this section I will complete the following key tasks:\n",
    "     1.      Check the data for errors.\n",
    "     2.      Transform the data so I can work with it effectively.\n",
    "     3.      Document the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2775176",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains when parsing with format \"%Y-%m-%d %H:%M:%S\": \".289\", at position 1694242. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1112\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m         values \u001b[38;5;241m=\u001b[39m convert_listlike(arg\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1113\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:488\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[1;32m    490\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[1;32m    491\u001b[0m     arg,\n\u001b[1;32m    492\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:519\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    509\u001b[0m     arg,\n\u001b[1;32m    510\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001b[0;32mstrptime.pyx:534\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:359\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unconverted data remains when parsing with format \"%Y-%m-%d %H:%M:%S\": \".289\", at position 1694242. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "df['started_at'] = pd.to_datetime(df['started_at'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc6ba016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride_id                      CDE6023BE6B11D2F\n",
      "rideable_type                   electric_bike\n",
      "started_at            2024-06-11 17:20:06.289\n",
      "ended_at              2024-06-11 17:21:39.464\n",
      "start_station_name                        NaN\n",
      "start_station_id                          NaN\n",
      "end_station_name                          NaN\n",
      "end_station_id                            NaN\n",
      "start_lat                               41.89\n",
      "start_lng                              -87.65\n",
      "end_lat                                 41.89\n",
      "end_lng                                -87.65\n",
      "member_casual                          casual\n",
      "Name: 1694242, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#investigating problematic row - started_at does not conform to datetime format\n",
    "problem_row = df.loc[1694242]\n",
    "print(problem_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4437978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count observations with fractional seconds in 'started_at'\n",
    "count_frxnal_seconds = df['started_at'].str.contains(r'\\.\\d+').sum()\n",
    "print(f\"Number of observations with fractional seconds in 'started_at' column: {count_frxnal_seconds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99eba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio of dataset that has fractional seconds\n",
    "ratio_frxnal_seconds = count_frxnal_seconds/(len(df))*100\n",
    "print(f\"Percent of dataset with fractional seconds: {ratio_frxnal_seconds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd06ce6",
   "metadata": {},
   "source": [
    "70% of the dataset has fractional seconds. Therefore, I cannot easily convert 'started_at' and 'ended_at' columns to datetime format. I will \n",
    "add .000 to the end of the 30% of the 'started_at'/'ended_at' that did not have fractional seconds and round. \n",
    "\n",
    "***For the purposes of this analysis this type of precision is not necessary. Removing the fractional seconds then converting to datetime would have sufficed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fractional_seconds(df):\n",
    "    \"\"\"\n",
    "    Adds '.000' to timestamps in 'started_at' and 'ended_at' columns that do not alread have fractional seconds.\n",
    "    \n",
    "    Args:\n",
    "        df: The pandas DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        The DataFrame with modified timestamps.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['started_at'] = df['started_at'].apply(lambda x: x+'.000' if not '.' in x else x)\n",
    "    df['ended_at'] = df['ended_at'].apply(lambda x: x+'.000' if not '.' in x else x)\n",
    "    return df\n",
    "\n",
    "#re-establish df\n",
    "df = add_fractional_seconds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 'started_at' and 'ended_at' columns to datetime\n",
    "df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "df['ended_at'] = pd.to_datetime(df['ended_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a43d3462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5682196 entries, 0 to 5682195\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 563.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#calculate and create a column called 'ride_length'\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of null values in each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# calculate the % of null values in each column\n",
    "null_percents = (null_counts/len(df)) * 100\n",
    "\n",
    "#create a DataFrame to display results\n",
    "null_summary = pd.DataFrame({'Null Count': null_counts,\n",
    "                            'Null Percentage': null_percents.round(2)})\n",
    "\n",
    "# print results\n",
    "print(\"Null Value Summary:\\n\", null_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows containing null values in end_lat and end_lng columns\n",
    "\n",
    "df = df.dropna(subset=['end_lat', 'end_lng'])\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f161d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_decimal_places(df, col_name):\n",
    "  \"\"\"\n",
    "  Counts the number of observations with each amount of decimal places \n",
    "  in the specified column of a DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df: The pandas DataFrame.\n",
    "    col_name: The name of the column to analyze.\n",
    "\n",
    "  Returns:\n",
    "    A pandas Series containing the counts of observations for each \n",
    "    number of decimal places.\n",
    "  \"\"\"\n",
    "\n",
    "  # Extract the decimal part of the column\n",
    "  decimal_parts = df['start_lat'].astype(str).str.split('.').str[1] \n",
    "\n",
    "  # Handle cases where there are no decimal places\n",
    "  decimal_parts = decimal_parts.fillna('') \n",
    "\n",
    "  # Count the number of decimal places for each observation\n",
    "  decimal_place_counts = decimal_parts.str.len() \n",
    "\n",
    "  # Count the occurrences of each decimal place count\n",
    "  decimal_place_counts = decimal_place_counts.value_counts()\n",
    "\n",
    "  return decimal_place_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c957ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_decimal_places(df, col_name):\n",
    "  \"\"\"\n",
    "  Counts the number of observations with each amount of decimal places \n",
    "  in the specified column of a DataFrame.\n",
    "\n",
    "  Args:\n",
    "    df: The pandas DataFrame.\n",
    "    col_name: The name of the column to analyze.\n",
    "\n",
    "  Returns:\n",
    "    A pandas Series containing the counts of observations for each \n",
    "    number of decimal places.\n",
    "  \"\"\"\n",
    "\n",
    "  # Extract the decimal part of the column\n",
    "  decimal_parts = df[col_name].astype(str).str.split('.').str[1] \n",
    "\n",
    "  # Handle cases where there are no decimal places\n",
    "  decimal_parts = decimal_parts.fillna('') \n",
    "\n",
    "  # Count the number of decimal places for each observation\n",
    "  decimal_place_counts = decimal_parts.str.len() \n",
    "\n",
    "  # Count the occurrences of each decimal place count\n",
    "  decimal_place_counts = decimal_place_counts.value_counts()\n",
    "\n",
    "  return decimal_place_counts\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "start_lat_decimal_counts = count_decimal_places(df, 'start_lat')\n",
    "end_lat_decimal_counts = count_decimal_places(df, 'end_lat')\n",
    "\n",
    "start_lng_decimal_counts = count_decimal_places(df, 'start_lng')\n",
    "end_lng_decimal_counts = count_decimal_places(df, 'end_lng')\n",
    "\n",
    "\n",
    "print(\"Decimal Place Counts for 'start_lat':\\n\", start_lat_decimal_counts)\n",
    "print(\"\\nDecimal Place Counts for 'end_lat':\\n\", end_lat_decimal_counts)\n",
    "\n",
    "print(\"Decimal Place Counts for 'start_lng':\\n\", start_lng_decimal_counts)\n",
    "print(\"\\nDecimal Place Counts for 'end_lng':\\n\", end_lng_decimal_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decided that for this analysis lat/lng data is sufficient, station name/id is not needed. Therefore I will drop 4 colummns: start_station_name, start_station_id, end_station_name, end_station_id\n",
    "\n",
    "df = df.drop(columns=['start_station_name', 'start_station_id', 'end_station_name', 'end_station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51546068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "\n",
    "#find duplicate rows based on all columns\n",
    "duplicate_rows = df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42299ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of duplicate rows\n",
    "num_duplicates = len(duplicate_rows)\n",
    "\n",
    "# calculate percentage duplicates make up of dataset \n",
    "percent_duplicates = num_duplicates / len(df) * 100\n",
    "\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "print(f\"% of dataset that are duplicates: {percent_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously had double imported august data. This was part of trouble shooting. Fixed the original import, this section no longer relevant.\n",
    "# duplicate_rows.groupby(['ride_id'])\n",
    "# print(duplicate_rows.groupby(['ride_id']).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadd66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed\n",
    "# check for duplicates\n",
    "\n",
    "# find duplicate rows based on ride_id column\n",
    "# duplicate_ride_ids = df['ride_id'].duplicated(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa48b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed \n",
    "# filter the DataFrame to include only rows with duplicate 'ride_id'\n",
    "# rows_with_duplicate_ride_ids = df[duplicate_ride_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed \n",
    "# sort the duplicate rows by 'ride_id'\n",
    "# rows_with_duplicate_ride_ids = rows_with_duplicate_ride_ids.sort_values(by='ride_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d429e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the sorted duplicate rows\n",
    "# print(rows_with_duplicate_ride_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0299ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_duplicate_ride_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_duplicate_ride_ids.groupby(['ride_id'])\n",
    "# print(rows_with_duplicate_ride_ids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_duplicate_ride_ids.groupby(['ride_id'])\n",
    "# print(rows_with_duplicate_ride_ids.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_duplicate = df[df['ride_id'] == '00001004F8DF01C8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37068ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the selected rows\n",
    "# print(example_duplicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8515b9",
   "metadata": {},
   "source": [
    "All duplicates seem to be in the month of August. Checking imported dfs to make sure August was not somehow uploaded twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed \n",
    "# df8.info()\n",
    "# df8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error fixed - cell not needed \n",
    "# print(len(df8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91a185",
   "metadata": {},
   "source": [
    "columns 10 (end_lat) and 11 (end_lng) have 754612 non-null values --> the same as number of duplicates listed. Must be a code error.\n",
    "\n",
    "Error identified in cell 20: original import brought in august twice and no september. Fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92693fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:220\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(left, right)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/computation/expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/computation/expressions.py:131\u001b[0m, in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m _evaluate_standard(op, op_str, a, b)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/computation/expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calculate ride length and add new column to dataframe\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mride_length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mended_at\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39msub)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:5819\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   5818\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 5819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:1381\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1381\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:285\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    281\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:229\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    223\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    224\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m         result \u001b[38;5;241m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:165\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 165\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# calculate ride length and add new column to dataframe\n",
    "\n",
    "df['ride_length'] = (df['ended_at'] - df['started_at'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate day_of_week and add column to dataframe\n",
    "df['day_of_week'] = df['started_at'].dt.day_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ab44409",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ride_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ride_length'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mride_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ride_length'"
     ]
    }
   ],
   "source": [
    "print(df['ride_length'].dtype())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ce05a",
   "metadata": {},
   "source": [
    "#### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73613c23",
   "metadata": {},
   "source": [
    "Save this to your data directory, separately. Uploaded raw data as monthly csvs. Should save derived data in a separate location. This guards against overwriting our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcda8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, filename, path):\n",
    "    \"\"\"\n",
    "    Saves a pandas DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        filename: The name of the CSV file.\n",
    "        path: The path to the directory where the file should be saved.\n",
    "    \"\"\"\n",
    "    filepath = f\"{path}/{filename}\" \n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to a new csv file\n",
    "datapath = '/Users/amylee/google_data_analytics/cyclistic_case_study/cyclistic_data'\n",
    "save_file(df, 'df_cyclistic_clean.csv', datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739a027",
   "metadata": {},
   "source": [
    "### PROCESS SUMMARY\n",
    "\n",
    "Data cleaning tasks completed in this section:\n",
    "1. Ensured all columns were in appropriate data format - converted 'started_at' and 'ended_at' to datetime\n",
    "2. Addressed/eliminated null values. \n",
    "a - Dropped rows with null values in 'end_lat' and 'end_lng' columns (~0.01% of dataset).\n",
    "b - Dropped 4 columns: 'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id'. 18% of observations in these columns were missing data. Dataset also contains start/end latitude/longitude so that will be used for geographical analysis. \n",
    "5. Checked for duplicates - none. Identified import error and corrected.\n",
    "\n",
    "I then added columns to calculate ride_length and the day_of_week for each ride."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
